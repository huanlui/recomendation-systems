{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "One of the most common uses of big data is to predict and suggest what users may want. \n",
    "This allows Google to show you relevant ads or to suggest news in Google Now; Amazon to recommend relevant products; Netflix to recommend movies that you might like; or most recently, the famous **Weekly Dicovery** of Spotify.\n",
    "\n",
    "All these products are based on systems of recommendation: a information retrieval method to provide users with relevant, yet novel and diverse, information. \n",
    "\n",
    "In this class we will use a pretty famous dataset based on movies ratings, 'MovieLens', to learn the basics of recommender systems. \n",
    "\n",
    "Table of Contents (times are approximated)\n",
    "\n",
    "1. [Getting and analysing some data (1.5-2 h, typically until break)](#data)\n",
    "2. [Most popular movies (0.5-1 min)](#popular)\n",
    "3. [Metrics for recommender systems (2-2.5h)](#metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 1.1 Load data\n",
    "\n",
    "We will use MovieLens dataset, which is one of the most common datasets used when implementing and testing recommender engines. This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens [website](https://movielens.org) during the seven-month period from September 19th, \n",
    "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
    "who had less than 20 ratings or did not have complete demographic\n",
    "information were removed from this data set.\n",
    "\n",
    "You can download the dataset [here](http://files.grouplens.org/datasets/movielens/ml-100k.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the readme file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"../data/ml-100k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat {data_root}README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ?\n",
    "datafile = os.path.join(data_root, \"u.data\")\n",
    "data = pd.read_csv(datafile, sep=?, names=columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A remainder of the numpy library\n",
    "\n",
    "*Pandas library is nothing alse than numpy under the hood (numpy with steroids, if you like). You can access the data (in matrix from) with he \"values\" attribute, e.g. data.values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The attribute shape provides the shape of the matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that if we return the first column, we get a vector (of 100000 components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with the first row (this time, we get a vector of 4 components)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access all rows, and first 3 columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access all collumns, and first 3 rows \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access first 10 rows\n",
    "\n",
    "# This is equivalent to data.values[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# access first column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of users and items\n",
    "n_users = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of items\n",
    "n_items = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"There are %s users and %s items\" %(n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A dictionary for movies and a search tool\n",
    "\n",
    "In order to analyze the predicted recommendations, let's create a python dictonary that will allow us to translate any item id to the corresponding movie title. Also, let's write a small function that returns the ids of the movies containing some text.\n",
    "\n",
    "The correspondance between titles and ids is stored in the u.item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"./../data/ml-100k/\"\n",
    "items_id_file = os.path.join(data_root, \"u.item\")\n",
    "!head $items_id_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Simple reminder of dictionaries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux = {'hola': 'que haces?', 1: '237'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access value of key='hola'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update value of existing key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keys and values at the same time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a dictionary for movie titles and ids\n",
    "item_dict = {}\n",
    "with io.open(items_id_file, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "        record = line.split(b'|')\n",
    "        item_dict[int(record[0])] = str(record[1])\n",
    "    \n",
    "# We can use this dict to see the films a user has seen, for instance. \n",
    "for record in data.values[:20]:\n",
    "    print(\"User {u} viewed '{m}' and gave a {r} rating\".format(\n",
    "        u=record[0], m=item_dict[record[1]], r=record[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves all the ids and titles for movies containing 'text' in its title\n",
    "def returnItemId(text, ids):\n",
    "    \"\"\"\n",
    "    :param text: string to be looked for in movies titles\n",
    "    :param ids: dicttionary of {id:title}\n",
    "    \n",
    "    :return: a list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "    \"\"\"\n",
    "    # convert input text to lowercase\n",
    "    text_ = text.lower()\n",
    "    # find occurances\n",
    "    search = []\n",
    "    for id_,title in ids.items():\n",
    "        ?\n",
    "    \n",
    "    return search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data consistency (always double check everything!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check whether titles are unique or not. They are not!!!\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One work around: create another dict that consolidates ids with the same movie title\n",
    "\n",
    "Alternatively, you could create a Dataframe from the above dictionary of ids:titles\n",
    "\n",
    "<pre><code>\n",
    "dd = pd.DataFrame.from_dict(item_dict, orient='index').reset_index()\n",
    "dd.columns = ['item_id', 'title']\n",
    "dd.head()\n",
    "</code></pre>\n",
    "\n",
    "find the duplicates (`dd.groupby('title')...`), and create a new id for the items. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates_item_dict = {}\n",
    "# Las claves en \"duplicates_item_dict\" son los nombres de las pel√≠culas\n",
    "# Los valores son una lista de los ids (que pueden ser uno solo, o varios)\n",
    "for id_, name in list(item_dict.items()):\n",
    "    # clave: name; valor: list(id_)\n",
    "    ?\n",
    "\n",
    "# show the duplicated titles\n",
    "for k,v in list(duplicates_item_dict.items()):\n",
    "    if len(v)>1:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict where the key are the original ids, and the values are the unique one. \n",
    "We will use this dictionary to remove duplicates in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_item_dict ={}\n",
    "for new_id, old_id_list in enumerate(duplicates_item_dict.values()):\n",
    "    # key: old_id; value: new_id\n",
    "    ?\n",
    "\n",
    "unique_id_item_dict = {old_id: new_id for new_id, old_id_list in enumerate(duplicates_item_dict.values()) \n",
    "                                      for old_id in old_id_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_id_item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dict mapping moving titles to this new unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_item_dict = {unique_id_item_dict[k]:v \n",
    "                    for k,v in item_dict.items()}\n",
    "    \n",
    "assert(len(set(unique_item_dict.keys())) == \n",
    "       len(set(unique_item_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `returnItemId()` mehtod safely =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnItemId('but', unique_item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train and test sets\n",
    "\n",
    "GroupLens provides several splits of the dataset, so that we can check the goodness of our algorithms. See the README file for more  details. Here we will use one of such splits.\n",
    "\n",
    "Please notice that we have to correct for the non-unique movie's id issue!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainfile = os.path.join(data_root, 'ua.base')\n",
    "!head $trainfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "trainfile = os.path.join(data_root, \"ua.base\")\n",
    "train = ?\n",
    "print('There are %s users, %s itmes and %s pairs in the train set' \\\n",
    "      %(train.user_id.unique().shape[0], train.item_id.unique().shape[0], train.item_id.count()))\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same for test\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "testfile = os.path.join(data_root, \"ua.test\")\n",
    "test = ?\n",
    "print('There are %s users, %s itmes and %s pairs in the test set' \\\n",
    "      %(test.user_id.unique().shape[0], test.item_id.unique().shape[0], test.item_id.count()))\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting for non-unique movies id \n",
    "\n",
    "Use Pandas `apply` method. Note it can be applied to all rows in a column, thus returning a Pandas Series; or it can be applied row-wise, he"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder of lambda functions in Python: is a way of calling short functions (1 line of code), without having to define the function in a separted cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['item_id'] = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['item_id'] = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='popular'></a>\n",
    "## 2. Most popular movies\n",
    "\n",
    "Recommending popular items is a simple, yet quite effective baseline for recommendation. Indeed, most RS suffer from a strong *popularity bias*, i.e. they tend to recommend popular items more frequently than they should -just because suggesting what is popular is effective!-. There is a lot of research  devote to understand this behaviour and to develop recipies to avoid it. \n",
    "\n",
    "Movies can be ranked according to different popularity metrics:\n",
    "* Most rated movie (it is assumed that this is the most watched movie)\n",
    "* Most positively rated movie (rating > 4.0)\n",
    "* Highest rated movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Most rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the train dataset by item and count the number of users using Pandas\n",
    "mostRated = train.groupby('item_id')['user_id'].count()\n",
    "mostRated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostRated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order\n",
    "mostRatedSorted = mostRated.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostRatedSorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the movie title, using the apply method, returning a Pandas Series in each row\n",
    "def add_tile(row):\n",
    "    id_ = ?\n",
    "    title_ = ?\n",
    "    freq_ = ?\n",
    "    new_row = ?\n",
    "    return pd.Series(new_row, index=['id', 'title', 'freq'])\n",
    "\n",
    "mostRatedMovies = (mostRatedSorted\n",
    "                   .reset_index()\n",
    "                   .apply(add_tile, axis=1)\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Most positively rated movie\n",
    "\n",
    "Use the query method to filter values in train, `train.query('rating>4')`, or the traditional form `train[train.rating>4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Highest mean rating movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the highest rated movies, with a minium number of users/ratings.\n",
    "min_ratings = 50\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: set the value of *min_ratings* to 1, and re-run the cell. What happens now? Change this value\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: Which method is better?? How to measure a recommender system? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** IMPORTANT QUESTION **: When might be useful to recommend popular items?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "## 3. Metrics for recommender systems\n",
    "\n",
    "As we have seen, even with the simplest solution --aka, recommending popular items-- is difficult to known which technique performs better. For this, there are a number of metrics that allow one to measure the goodness of a recommender system. \n",
    "\n",
    "Metrics can be design for measuring the relevance or accuracy of a recommendation, but they can be created for evaluating the novelty of a recommendation, or its diversity. \n",
    "\n",
    "For now, we will focus on relevance and accuracy. Several metrics exist:\n",
    "* Accuracy: rmse, mae.\n",
    "* Not ranked: Recall@k, Precision@k.\n",
    "* With rank disccount: map@k, ndcg@k.\n",
    "* With rank ordering: mean percentile rank.\n",
    "\n",
    "We will be definiing some of them whitin this class. For the moment, let's talk about precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Precision and recall\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision and Recall in IR\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "The concept of precision and recall comes form the world of information retrieval, have a look at the wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "From this entry:\n",
    "\n",
    " * \"**precision** (also called positive predictive value) is the fraction of retrieved instances that are relevant\".\n",
    " * \"**recall** (also known as sensitivity) is the fraction of relevant instances that are retrieved\".\n",
    "\n",
    "<br />\n",
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: how do we know if some movie, unknown to the user, is relevant?\n",
    "</div>\n",
    "\n",
    "In other words, we cannot measure a false positive --something recommended that was not relevant--. In this regard, only recall-oriented metrics have an actual meaning in RS. Nonetheless, its common practice to define both metrics in RS as follows:\n",
    " \n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{k=1}^N rel(k)}{\\sum_{i\\in \\mathcal{I}_u} 1}$$\n",
    "### $$\\mathrm{precision}@N = \\frac{\\sum_{k=1}^N rel(k)}{N}$$\n",
    "\n",
    "Here, $\\mathcal{I}_u$ is the set of items adopted by user $u$, and $rel(k)$ is the relevance of a recommendation at position k in the list of recommendations. For ratings, the relevance could be defined as those movies rated above a certain threshold, e.g. $r_{ui}>4.0$. \n",
    "\n",
    "**Important to note: since precision is pretty much the same as recall in RS, metrcis usch as the *area under the ROC curve* doesn't have any meaning!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "As an example, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />\n",
    "**What would be the recall and precision @1? and @2?**\n",
    "<br />\n",
    "**What do you think of recommending Batman? Is a bad or a good recommendation?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that there isn't any actual difference between precision and recall in the context of RS: both measure the relevance of the recommendations, and tell nothing about items recommended that haven't been adopted by the user. Thus, it make sense to define a normalized recall as:\n",
    "\n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{i=1}^N rel_i}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1})$$\n",
    "\n",
    "This way, results are normalized to 1 always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Exercise** Implement the above definition of recall\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_n(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return the recall\n",
    "    \"\"\"\n",
    "    if train is not None: # Remove items in train\n",
    "        rec_true = []\n",
    "        for r in recommended:\n",
    "            ?\n",
    "    else:\n",
    "        rec_true = recommended    \n",
    "    intersection = ?\n",
    "    return intersection / float(np.minimum(N, len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Batman (1989)', 'Madame Butterfly (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_n(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall_at_n(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check it's well normalized\n",
    "print(recall_at_n(3, seen, recommended))\n",
    "print(recall_at_n(10, seen, recommended))\n",
    "print(recall_at_n(100, seen, recommended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use this implementation to measure the efficiency of the popularity baselines in the test set. Use the top-5 movies, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mostRatedMovies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveRatedMovies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanRateMovies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since `recall_at_n` takes both train and test list per user, we need to create a dataset with the list of movies seen in train and test*\n",
    "\n",
    "Thus, get the list of movies per user in train and test, and join the two dataframes. For the join, use the pandas method `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get movies in train per user. For this, group by user and get a list of item ids.\n",
    "trainUsersGrouped = ?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same with test data\n",
    "testUsersGrouped = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the join: use pandas merge method\n",
    "joined = ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined.item_id_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How would you access values in test?\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This second method is easier if we want to access several columns at once, and operate over them.\n",
    "# For instance, if we like to concatenate both train and test list, we will do:\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the above method to calculate the recall of the mostRatedMovies recommendation, for each user:\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As you can see, some users have a quite large recall (0.5), while for others is small (e.g, 0.14). Let's calculate the mean.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 30\n",
    "# calculate the average recall across all users for mostRatedMovies recommendation\n",
    "recall_per_user = ?\n",
    "recall_per_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average recall across all users for positiveRatedMovies recommendation\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the average recall across all users for meanRatedMovies recommendation\n",
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mean Averaged Precision (MAP) -- Advanced material\n",
    "\n",
    "Previous metrics did not account for the ranking of the recommendation, i.e. the relative position of a movie within the sorted list of recommendations. **But orders matters!** Metrics like MAP, MRR or NDCG try to tackle down this problem. \n",
    "\n",
    "From the blog *http://fastml.com/what-you-wanted-to-know-about-mean-average-precision/*:\n",
    "\n",
    "> Here‚Äôs another way to understand average precision. Wikipedia says AP is used to score document retrieval. You can think of it this way: you type something in Google and it shows you 10 results. It‚Äôs probably best if all of them were relevant. If only some are relevant, say five of them, then it‚Äôs much better if the relevant ones are shown first. It would be bad if first five were irrelevant and good ones only started from sixth, wouldn‚Äôt it? AP score reflects this.\n",
    "\n",
    "Implementation taken from:\n",
    "\n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision \n",
    "\n",
    "The Average Precision is definied as:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N P(k) \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n",
    "where $P(k)$ is the precision at cut-off in the item list, i.e. the ratio of the number of recommended items adopted, up to the position k, over the number k. Thus:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N \\left(\\sum_{i=1}^k rel(i)\\right)/k \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Following the example above, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**Calculate AP@1**\n",
    "<br /><br />\n",
    "First, *rel(1)=0*, because Batman was not viewed. Also, *P(1) = 0*. Thus, AP@1=0.\n",
    "<br />\n",
    "**Calculate AP@2**\n",
    "<br /><br />\n",
    "As before, *rel(1)=0*, so the first term does not contribute. For the second term, *rel(2)=1*, so that *P(2)=0.5*. The numerator is hence:\n",
    "<br /><br />\n",
    "$P(1)*rel(1)+P(2)*rel(2)=0*0+0.5*1$\n",
    "<br /><br />\n",
    "For the denominator, $N=2$ and $\\sum_{i\\in \\mathcal{I}_u} 1)=4$, thus:\n",
    "<br /><br />\n",
    "AP@2 = 0.5/2 = 0.25\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement it =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    Computes the average precision at N given recommendations.\n",
    "    \n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return The average precision at N over the test set\n",
    "    \"\"\"\n",
    "    if train is not None: \n",
    "        rec_true = []\n",
    "        for r in recommended:\n",
    "            if r not in train:\n",
    "                rec_true.append(r)\n",
    "    else:\n",
    "        rec_true = recommended    \n",
    "    predicted = rec_true[:N] # top-k predictions\n",
    "    \n",
    "    score = 0.0 # This will store the numerator\n",
    "    num_hits = 0.0 # This will store the sum of rel(i)\n",
    "\n",
    "    for i,p in enumerate(predicted):\n",
    "        if p in test and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits/(i+1.0)\n",
    "\n",
    "    return score / min(len(test), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Madame Butterfly (1995)', 'Batman (1989)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apk(3, seen, recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    "\n",
    "Mean avergae precision is nothing else than the AP averaged across users ;)\n",
    "\n",
    "Apply it to popularity baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "The rest of the class is covered in a different notebook\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
